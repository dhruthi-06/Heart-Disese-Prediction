# -*- coding: utf-8 -*-
"""Heart_disease_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FljC9hdIc_bUa5ctNTH5rC9tqfxr762-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

import os
print(os.listdir())

import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv("heart.csv")

type(data)

data.shape

data.head()

data.describe()

data.info()

data.sample(5)

data.isnull().sum()

data.isnull().sum().sum()

print(data.corr()["target"].abs().sort_values(ascending=False))

y = data["target"]

ax = sns.countplot(data=data, x="target")
target_temp = data.target.value_counts()
print(target_temp)
total = target_temp.sum()
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x() + p.get_width() / 2.,
            height + 3,
            '{:1.2f}%'.format(height / total * 100),
            ha='center')

plt.show()

print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/303,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/303,2)))

data["sex"].unique()

ax = sns.barplot(x=data["sex"], y=data["target"])
plt.show()

def plotAge(data, axes):
    sns.kdeplot(data=data, x="age", hue="target", shade=True, ax=axes[0])
    legend_labels = ['disease false', 'disease true']

    legend = axes[0].get_legend()
    for t, l in zip(legend.texts, legend_labels):
        t.set_text(l)
    axes[0].set(xlabel='age', ylabel='density', title='Age Distribution by Target')

    avg = data[["age", "target"]].groupby(['age'], as_index=False).mean()
    sns.barplot(x='age', y='target', data=avg, ax=axes[1])
    axes[1].set(xlabel='age', ylabel='disease probability', title='Average Disease Probability by Age')

    plt.tight_layout()
    plt.show()

fig_age, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 8))
plotAge(data, axes)

countFemale = len(data[data.sex == 0])
countMale = len(data[data.sex == 1])
print("Percentage of Female Patients:{:.2f}%".format((countFemale)/(len(data.sex))*100))
print("Percentage of Male Patients:{:.2f}%".format((countMale)/(len(data.sex))*100))

categorial = [('sex', ['female', 'male']),
              ('cp', ['typical angina', 'atypical angina', 'non-anginal pain', 'asymptomatic']),
              ('fbs', ['fbs > 120mg', 'fbs < 120mg']),
              ('restecg', ['normal', 'ST-T wave', 'left ventricular']),
              ('exang', ['yes', 'no']),
              ('slope', ['upsloping', 'flat', 'downsloping']),
              ('thal', ['normal', 'fixed defect', 'reversible defect'])]

def plotGrid(isCategorial):
    if isCategorial:
        [plotCategorial(x[0], x[1], i) for i, x in enumerate(categorial)]
    else:
        [plotContinuous(x[0], x[1], i) for i, x in enumerate(continuous)]

def plotCategorial(attribute, labels, ax_index):
    sns.countplot(x=attribute, data=data, ax=axes[ax_index][0])
    sns.countplot(x='target', hue=attribute, data=data, ax=axes[ax_index][1])
    avg = data[[attribute, 'target']].groupby([attribute], as_index=False).mean()
    sns.barplot(x=attribute, y='target', hue=attribute, data=avg, ax=axes[ax_index][2])

    for t, l in zip(axes[ax_index][1].get_legend().texts, labels):
        t.set_text(l)
    for t, l in zip(axes[ax_index][2].get_legend().texts, labels):
        t.set_text(l)

fig_categorial, axes = plt.subplots(nrows=len(categorial), ncols=3, figsize=(15, 30))

plotGrid(isCategorial=True)

continuous = [('trestbps', 'blood pressure in mm Hg'),
              ('chol', 'serum cholestoral in mg/d'),
              ('thalach', 'maximum heart rate achieved'),
              ('oldpeak', 'ST depression by exercise relative to rest'),
              ('ca', '# major vessels: (0-3) colored by flourosopy')]

def plotContinuous(attribute, xlabel, ax_index):
    sns.distplot(data[[attribute]], ax=axes[ax_index][0])
    axes[ax_index][0].set(xlabel=xlabel, ylabel='density')
    sns.violinplot(x='target', y=attribute, data=data, ax=axes[ax_index][1])

fig_continuous, axes = plt.subplots(nrows=len(continuous), ncols=2, figsize=(15, 22))

plotGrid(isCategorial=False)

pd.crosstab(data.age,data.target).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.savefig('heartDiseaseAndAges.png')
plt.show()

pd.crosstab(data.sex,data.target).plot(kind="bar",figsize=(20,10),color=['blue','#AA1111' ])
plt.title('Heart Disease Frequency for Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.xticks(rotation=0)
plt.legend(["Don't have Disease", "Have Disease"])
plt.ylabel('Frequency')
plt.show()

data.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',
       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']

data.head()

pd.crosstab(data.fasting_blood_sugar,data.target).plot(kind="bar",figsize=(20,10),color=['#4286f4','#f49242'])
plt.title("Heart disease according to FBS")
plt.xlabel('FBS- (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')
plt.xticks(rotation=90)
plt.legend(["Don't Have Disease", "Have Disease"])
plt.ylabel('Disease or not')
plt.show()

data["chest_pain_type"].unique()

plt.figure(figsize=(26, 10))
sns.barplot(x=data["chest_pain_type"], y=data["target"])

plt.xlabel('Chest Pain Type')
plt.ylabel('Target')
plt.title('Bar Plot of Chest Pain Type vs. Target')
plt.show()

data["resting_blood_pressure"].unique()

agg_data = data.groupby('resting_blood_pressure')['target'].mean().reset_index()

plt.figure(figsize=(26, 10))
sns.barplot(x='resting_blood_pressure', y='target', data=agg_data)

plt.xlabel('Resting Blood Pressure')
plt.ylabel('Mean Target')
plt.title('Mean Target by Resting Blood Pressure')
plt.show()

data["rest_ecg"].unique()

y = data["target"]

plt.figure(figsize=(26, 15))
sns.barplot(x=data["rest_ecg"], y=y)

plt.xlabel('Rest ECG')
plt.ylabel('Target')
plt.title('Bar Plot of Rest ECG vs. Target')
plt.show()

data["exercise_induced_angina"].unique()

plt.figure(figsize=(10, 10))
sns.barplot(x=data["exercise_induced_angina"], y=y)

plt.xlabel('Exercise Induced Angina')
plt.ylabel('Target')
plt.title('Bar Plot of Exercise Induced Angina vs. Target')
plt.show()

data["st_slope"].unique()

plt.figure(figsize=(25, 10))
sns.barplot(x=data["st_slope"], y=y)

plt.xlabel('ST Slope')
plt.ylabel('Target')
plt.title('Bar Plot of ST Slope vs. Target')
plt.show()

data["num_major_vessels"].unique()

sns.countplot(data["num_major_vessels"])
plt.xlabel('Number of Major Vessels')
plt.ylabel('Count')
plt.title('Count Plot of Number of Major Vessels')
plt.show()

sns.barplot(x=data["num_major_vessels"], y=y)

plt.xlabel('Number of Major Vessels')
plt.ylabel('Target')
plt.title('Bar Plot of Number of Major Vessels vs. Target')
plt.show()

data["thalassemia"].unique()

sns.distplot(data["thalassemia"])

sns.barplot(x=data["thalassemia"], y=y)

plt.xlabel('Thalassemia')
plt.ylabel('Target')
plt.title('Bar Plot of Thalassemia vs. Target')
plt.show()

plt.figure(figsize=(20,10))
sns.scatterplot(x='cholesterol',y='thalassemia',data=data,hue='target')
plt.show()

plt.figure(figsize=(20,10))
sns.scatterplot(x='thalassemia',y='resting_blood_pressure',data=data,hue='target')
plt.show()

plt.figure(figsize=(20, 10))
plt.scatter(x=data.age[data.target==1], y=data.thalassemia[(data.target==1)], c="green")
plt.scatter(x=data.age[data.target==0], y=data.thalassemia[(data.target==0)])
plt.legend(["Disease", "Not Disease"])
plt.xlabel("Age")
plt.ylabel("Maximum Heart Rate")
plt.show()

sns.pairplot(data=data)

data.hist()

cnames=['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression','num_major_vessels']

f, ax = plt.subplots(figsize=(7, 5))

df_corr = data.loc[:,cnames]

corr = df_corr.corr()

sns.heatmap(corr, annot = True, cmap='coolwarm',linewidths=.1)
plt.show()

df_corr = data.loc[:,cnames]
df_corr

from sklearn.model_selection import train_test_split

predictors = data.drop("target",axis=1)
target = data["target"]

X_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)
print("Training features have {0} records and Testing features have {1} records.".\
      format(X_train.shape[0], X_test.shape[0]))

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

from sklearn.metrics import accuracy_score

def train_model(X_train, y_train, X_test, y_test, classifier, **kwargs):

    """
    Fit the chosen model and print out the score.

    """

    # instantiate model
    model = classifier(**kwargs)

    # train model
    model.fit(X_train,y_train)

    # check accuracy and print out the results
    fit_accuracy = model.score(X_train, y_train)
    test_accuracy = model.score(X_test, y_test)

    print(f"Train accuracy: {fit_accuracy:0.2%}")
    print(f"Test accuracy: {test_accuracy:0.2%}")

    return model

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

logreg.fit(X_train, Y_train)

y_pred_lr = logreg.predict(X_test)
print(y_pred_lr)

score_lr = round(accuracy_score(y_pred_lr,Y_test)*100,2)

print("The accuracy score achieved using Logistic Regression is: "+str(score_lr)+" %")

from sklearn.linear_model import LogisticRegression
model = train_model(X_train, Y_train, X_test, Y_test, LogisticRegression)

clf = LogisticRegression(random_state=0, solver='newton-cg', multi_class='multinomial').fit(X_test, Y_test)
#The solver for weight optimization.
#'lbfgs' is an optimizer in the family of quasi-Newton methods.
clf.score(X_test, Y_test)

from sklearn.metrics import confusion_matrix

matrix= confusion_matrix(Y_test, y_pred_lr)

sns.heatmap(matrix,annot = True, fmt = "d")

from sklearn.metrics import precision_score

precision = precision_score(Y_test, y_pred_lr)

print("Precision: ",precision)

from sklearn.metrics import recall_score

recall = recall_score(Y_test, y_pred_lr)

print("Recall is: ",recall)

print((2*precision*recall)/(precision+recall))

CM =pd.crosstab(Y_test, y_pred_lr)
CM

TN=CM.iloc[0,0]
FP=CM.iloc[0,1]
FN=CM.iloc[1,0]
TP=CM.iloc[1,1]

fnr=FN*100/(FN+TP)
fnr

from sklearn.ensemble import RandomForestClassifier
randfor = RandomForestClassifier(n_estimators=100, random_state=0)

randfor.fit(X_train, Y_train)

y_pred_rf = randfor.predict(X_test)
print(y_pred_rf)

from sklearn.model_selection import learning_curve
# Create CV training and test scores for various training set sizes
train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(),
                                                        X_train,
                                                        Y_train,
                                                        # Number of folds in cross-validation
                                                        cv=10,
                                                        # Evaluation metric
                                                        scoring='accuracy',
                                                        # Use all computer cores
                                                        n_jobs=-1,
                                                        # 50 different sizes of the training set
                                                        train_sizes=np.linspace(0.01, 1.0, 50))

# Create means and standard deviations of training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Create means and standard deviations of test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Draw lines
plt.plot(train_sizes, train_mean, '--', color="#111111",  label="Training score")
plt.plot(train_sizes, test_mean, color="#111111", label="Cross-validation score")

# Draw bands
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDDDD")

# Create plot
plt.title("Learning Curve")
plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc="best")
plt.tight_layout()
plt.show()

score_rf = round(accuracy_score(y_pred_rf,Y_test)*100,2)

print("The accuracy score achieved using Random Forest is: "+str(score_rf)+" %")

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=0)
rf.fit(X_train, Y_train)
print("Accuracy on training set: {:.3f}".format(rf.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(rf.score(X_test, Y_test)))

rf1 = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=0)
rf1.fit(X_train, Y_train)
print("Accuracy on training set: {:.3f}".format(rf1.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(rf1.score(X_test, Y_test)))

from sklearn.metrics import confusion_matrix

matrix= confusion_matrix(Y_test, y_pred_rf)

sns.heatmap(matrix,annot = True, fmt = "d")

from sklearn.metrics import precision_score

precision = precision_score(Y_test, y_pred_rf)

print("Precision: ",precision)

from sklearn.metrics import recall_score

recall = recall_score(Y_test, y_pred_rf)

print("Recall is: ",recall)

print((2*precision*recall)/(precision+recall))

CM = pd.crosstab(Y_test, y_pred_rf)
CM

TN=CM.iloc[0,0]
FP=CM.iloc[0,1]
FN=CM.iloc[1,0]
TP=CM.iloc[1,1]

fnr=FN*100/(FN+TP)
fnr

from sklearn.naive_bayes import GaussianNB
nb = train_model(X_train, Y_train, X_test, Y_test, GaussianNB)

nb.fit(X_train, Y_train)

y_pred_nb = nb.predict(X_test)
print(y_pred_nb)

score_nb = round(accuracy_score(y_pred_nb,Y_test)*100,2)

print("The accuracy score achieved using Naive Bayes is: "+str(score_nb)+" %")

from sklearn.naive_bayes import GaussianNB
model = train_model(X_train, Y_train, X_test, Y_test, GaussianNB)

from sklearn.metrics import confusion_matrix

matrix= confusion_matrix(Y_test, y_pred_nb)

sns.heatmap(matrix,annot = True, fmt = "d")

from sklearn.metrics import precision_score

precision = precision_score(Y_test, y_pred_nb)

print("Precision: ",precision)

from sklearn.metrics import recall_score

recall = recall_score(Y_test, y_pred_nb)

print("Recall is: ",recall)

print((2*precision*recall)/(precision+recall))

CM = pd.crosstab(Y_test, y_pred_nb)
CM

TN=CM.iloc[0,0]
FP=CM.iloc[0,1]
FN=CM.iloc[1,0]
TP=CM.iloc[1,1]

fnr = FN*100/(FN+TP)
fnr

from sklearn.neighbors import KNeighborsClassifier
knn = train_model(X_train, Y_train, X_test, Y_test, KNeighborsClassifier, n_neighbors=8)

knn.fit(X_train, Y_train)

y_pred_knn = knn.predict(X_test)
print(y_pred_knn)

score_knn = round(accuracy_score(y_pred_knn,Y_test)*100,2)

print("The accuracy score achieved using KNN is: "+str(score_knn)+" %")

from sklearn.neighbors import KNeighborsClassifier
model = train_model(X_train, Y_train, X_test, Y_test, KNeighborsClassifier)

for i in range(1,10):
    print("n_neigbors = "+str(i))
    train_model(X_train, Y_train, X_test, Y_test, KNeighborsClassifier, n_neighbors=i)

from sklearn.metrics import confusion_matrix

matrix= confusion_matrix(Y_test, y_pred_knn)

sns.heatmap(matrix,annot = True, fmt = "d")

from sklearn.metrics import precision_score

precision = precision_score(Y_test, y_pred_knn)

print("Precision: ",precision)

from sklearn.metrics import recall_score

recall = recall_score(Y_test, y_pred_knn)

print("Recall is: ",recall)

print((2*precision*recall)/(precision+recall))

CM = pd.crosstab(Y_test, y_pred_knn)
CM

TN=CM.iloc[0,0]
FP=CM.iloc[0,1]
FN=CM.iloc[1,0]
TP=CM.iloc[1,1]

fnr = FN*100/(FN+TP)
fnr

CM = pd.crosstab(Y_test, y_pred_knn)
TN=CM.iloc[0,0]
FP=CM.iloc[0,1]
FN=CM.iloc[1,0]
TP=CM.iloc[1,1]
fnr = FN*100/(FN+TP)
fnr

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(max_depth=3, random_state=0)

dt.fit(X_train, Y_train)

y_pred_dt = dt.predict(X_test)
print(y_pred_dt)

score_dt = round(accuracy_score(y_pred_dt,Y_test)*100,2)

print("The accuracy score achieved using Decision Tree is: "+str(score_dt)+" %")

from sklearn.tree import DecisionTreeClassifier
tree1 = DecisionTreeClassifier(random_state=0)
tree1.fit(X_train, Y_train)
print("Accuracy on training set: {:.3f}".format(tree1.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(tree1.score(X_test, Y_test)))

tree1 = DecisionTreeClassifier(max_depth=3, random_state=0)
tree1.fit(X_train, Y_train)
print("Accuracy on training set: {:.3f}".format(tree1.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(tree1.score(X_test, Y_test)))

df = pd.read_csv('heart.csv')

df.head()

pip install pydotplus

from pandas import DataFrame, Series
from IPython.display import Image
from io import StringIO
import pydotplus
from sklearn import preprocessing

def plot_decision_tree(clf,feature_name,target_name):
    dot_data = StringIO()
    tree.export_graphviz(clf, out_file=dot_data,
                         feature_names=feature_name,
                         filled=True, rounded=True,
                         special_characters=True)
    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
    return Image(graph.create_png())

from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train,Y_train)

plot_decision_tree(clf, X_train.columns,df.columns[1])

from sklearn import tree
HOW_DEEP_TREES = 1
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=HOW_DEEP_TREES)
clf = clf.fit(X_train, Y_train)
clf

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,
                             max_features=None, max_leaf_nodes=None,
                             min_impurity_decrease=0.0, min_samples_leaf=1,
                             min_samples_split=2, min_weight_fraction_leaf=0.0,
                             random_state=0, splitter='best')

print(clf)

from sklearn.tree import DecisionTreeClassifier

# Create DecisionTreeClassifier instance
clf = DecisionTreeClassifier()

# Fit the classifier to the training data
clf.fit(X_train, Y_train)

plot_decision_tree(clf, X_train.columns,df.columns[1])

from sklearn.metrics import confusion_matrix

matrix= confusion_matrix(Y_test, y_pred_dt)

sns.heatmap(matrix,annot = True, fmt = "d")

from sklearn.metrics import precision_score

precision = precision_score(Y_test, y_pred_dt)

print("Precision: ",precision)

from sklearn.metrics import recall_score

recall = recall_score(Y_test, y_pred_dt)

print("Recall is: ",recall)

print((2*precision*recall)/(precision+recall))

CM = pd.crosstab(Y_test, y_pred_dt)
CM

TN=CM.iloc[0,0]
FP=CM.iloc[0,1]
FN=CM.iloc[1,0]
TP=CM.iloc[1,1]
fnr = FN*100/(FN+TP)
fnr

accuracy = []

# list of algorithms names
classifiers = ['KNN', 'Decision Trees', 'Logistic Regression', 'Naive Bayes', 'Random Forests']

# list of algorithms with parameters
models = [KNeighborsClassifier(n_neighbors=8), DecisionTreeClassifier(max_depth=3, random_state=0), LogisticRegression(),
        GaussianNB(), RandomForestClassifier(n_estimators=100, random_state=0)]

# loop through algorithms and append the score into the list
for i in models:
    model = i
    model.fit(X_train, Y_train)
    score = model.score(X_test, Y_test)
    accuracy.append(score)

summary = pd.DataFrame({'accuracy':accuracy}, index=classifiers)
summary

sns.barplot(x=algorithms, y=scores)
scores = [score_lr, score_nb, score_knn, score_dt, score_rf]
algorithms = ["Logistic Regression", "Naive Bayes", "K-Nearest Neighbors", "Decision Tree", "Random Forest"]
sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")
sns.barplot(x=algorithms, y=scores)